{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "cc387792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import  ABC, abstractmethod\n",
    "\n",
    "class Readable(ABC):\n",
    "    def __init__(self): pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get(): pass\n",
    "\n",
    "class Writable(ABC):\n",
    "    def __init__(self): pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def put(): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17641b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import os\n",
    "from fake_useragent import UserAgent\n",
    "from typing import Dict\n",
    "class WorkerWithHtml(Readable):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Объявление переменных, которые будут использоваться этим Worker-ом\n",
    "        self._url = None\n",
    "        self._user = None\n",
    "        self.__header = None\n",
    "\n",
    "    async def get(self, \n",
    "                  session: aiohttp.ClientSession, \n",
    "                  url: str, \n",
    "                  accept: str= '*/*',\n",
    "                  header: Dict[str, str]= None):\n",
    "        \n",
    "        self._url = url\n",
    "        self._user = UserAgent().random\n",
    "        \n",
    "        if not header:\n",
    "            self.__header = {'Accept' : accept, \n",
    "                             'User-Agent': self._user}\n",
    "        else:\n",
    "            self.__header = header\n",
    "\n",
    "        # Чтение данных из сайта\n",
    "        data = None\n",
    "        async with session.get(url= self._url, \n",
    "                               headers= self.__header) as response:\n",
    "            data = await response.text()\n",
    "\n",
    "        return data\n",
    "    \n",
    "    @property\n",
    "    def user(self):\n",
    "        return  self._user\n",
    "\n",
    "    async def _read_main_site(self, path_main_site: str, url):\n",
    "\n",
    "        # Забираем данные с основного сайта и кладем их в файла \n",
    "        data = None\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            data = await self.get(session, url= url)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "145158bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiofiles\n",
    "from typing import Any\n",
    "import json\n",
    "\n",
    "class WorkerWithFiles(Readable, Writable):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    async def get(self, path: str):\n",
    "        data = None\n",
    "        async with aiofiles.open(file= path) as file:\n",
    "            data = await file.read()\n",
    "        return data\n",
    "    \n",
    "    async def put(self, path: str, data: Any):\n",
    "        async with aiofiles.open(file= path, mode= 'w') as file:\n",
    "            await file.write(data)\n",
    "    \n",
    "    async def _put_json_file(self, path: str, data: Any):\n",
    "        async with aiofiles.open(path, 'w') as file:\n",
    "            json_str = json.dumps(obj= data, indent= 4, ensure_ascii= False) \n",
    "            await file.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "6206725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Parser(ABC):\n",
    "    def __init__(self, base_url: str):\n",
    "        self.__file_worker = WorkerWithFiles()\n",
    "        self.__html_worker = WorkerWithHtml()\n",
    "        self.base_url = base_url\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def parsing(): pass\n",
    "\n",
    "    @property\n",
    "    def user_agent(self):\n",
    "        return self.__html_worker.user\n",
    "    \n",
    "    async def put_file(self, path: str, data: Any):\n",
    "        await self.__file_worker.put(path= path, data= data)\n",
    "    \n",
    "\n",
    "    async def get_file(self, path: str):\n",
    "        return await self.__file_worker.get(path= path)\n",
    "\n",
    "    \n",
    "    async def get_html(self, \n",
    "                       session: aiohttp.ClientSession,\n",
    "                       url: str, \n",
    "                       accept: str= '*/*',\n",
    "                       with_delay= True, \n",
    "                       header: Dict[str, str]= None):\n",
    "        return await self.__html_worker.get(session= session, url= url, accept= accept, with_delay= with_delay, header= header)\n",
    "    \n",
    "\n",
    "    async def _read_main_site_and_save(self, path_main_site: str, file_name_for_main_site: str):\n",
    "        \n",
    "        data = await self.__html_worker._read_main_site(file_name_for_main_site, self.base_url)\n",
    "        \n",
    "        # сохраняет данные в файл\n",
    "        if BeautifulSoup(data, 'lxml').find(name= 'title').text == \"Слишком много запросов\":\n",
    "            raise Exception(\"Вас заблокировали!\")\n",
    "        \n",
    "        path= os.path.join(path_main_site, file_name_for_main_site)\n",
    "        await self.put_file(path= path, data= data)\n",
    "        \n",
    "\n",
    "    async def _save_data_in_json_file(self, path: str, data: Any):\n",
    "        await self.__file_worker._put_json_file(path, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ce4a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParserSite_23MET(Parser):\n",
    "    def __init__(self, base_url):\n",
    "        super().__init__(base_url)\n",
    "\n",
    "    async def __get_hrefs_for_next_sites(self,\n",
    "                                         file_path: str):\n",
    "\n",
    "        #сначало парсим основной сайт\n",
    "        html_file = await self.get_file(path= file_path)\n",
    "        soup = BeautifulSoup(markup= html_file, \n",
    "                             features= 'lxml')\n",
    "        \n",
    "        items_html_with_href= soup.find(name= 'nav', \n",
    "                                        attrs={\"id\" : \"left-container\", \"class\" : \"left-container-mainpage-static\"})\\\n",
    "                                  .find(name= 'ul')\\\n",
    "                                  .find_all(name= 'a')\n",
    "        \n",
    "        # формируем json с ссылками на сайты, внутри которых будут еще одни сайты с сылками на данные\n",
    "        hrefs_next_sites = dict()\n",
    "        for item in items_html_with_href:\n",
    "            item: BeautifulSoup\n",
    "            item_name = item.get_text()\n",
    "            item_href = self.base_url + item.get('href').replace('/price', '')\n",
    "            hrefs_next_sites[item_name] = item_href\n",
    "\n",
    "        return hrefs_next_sites\n",
    "\n",
    "    # async def __sub_get_hrefs_for_next_sites(self, \n",
    "    #                                          file_path: str):\n",
    "    #     html_file = await self.get_file(path= file_path)\n",
    "\n",
    "    async def parsing(self,\n",
    "                      file_name_for_main_site= 'main_site.html',\n",
    "                      accept= '*/*',\n",
    "                      dir_name= None,\n",
    "                      with_save_data_in_file= True): \n",
    "\n",
    "#Дальше реализовать два способа реализации parsing-а c сохранением всех данных в файлы и брать записи из этих файлов... \n",
    "        \n",
    "        path = os.getcwd()\n",
    "\n",
    "        # создаю директорию, если есть dir_name\n",
    "        if dir_name:\n",
    "            path = os.path.join(path, dir_name)\n",
    "            if not os.path.isdir(path):\n",
    "                os.mkdir(path= path)\n",
    "                print(f\"Создал папку {dir_name} по пути {path}\")\n",
    "            else:\n",
    "                print(f\"Не стал создавать папку {dir_name}, т.к она уже существует!\")\n",
    "        \n",
    "        try:\n",
    "            await self._read_main_site_and_save(path_main_site= path, \n",
    "                                                file_name_for_main_site= file_name_for_main_site)\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            return ex\n",
    "\n",
    "        # Получаем json с ссылками на сайты\n",
    "        hrefs_next_sites = await self.__get_hrefs_for_next_sites(os.path.join(path, file_name_for_main_site)) \n",
    "        \n",
    "        # # Сохраним полученный json в файл\n",
    "        # await self._save_data_in_json_file(path= os.path.join(path, file_name_for_main_site.split('.')[0] + '.json'),\n",
    "        #                                    data= hrefs_next_sites)\n",
    "\n",
    "        # Переходим по ссылкам\n",
    "        header = {'Accept' : accept,\n",
    "                  'User-agent' : self.user_agent}\n",
    "        \n",
    "        tasks = []\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            for submain_url in hrefs_next_sites.values():\n",
    "                task =  asyncio.create_task(self.get_html(session= session,\n",
    "                                                          url= submain_url,\n",
    "                                                          header= header)) \n",
    "                tasks.append(task)\n",
    "            html_codes = await asyncio.gather(*tasks)\n",
    "        \n",
    "        if with_save_data_in_file:\n",
    "            # Сохраняем все полученные данные в файлы\n",
    "            for i, detail_name in enumerate(hrefs_next_sites.keys()):\n",
    "                await self.put_file(path= os.path.join(path, detail_name),\n",
    "                                    data= html_codes[i])\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "4f9fc26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создал папку data по пути /home/ranil/Рабочий стол/Project/ПРОЕКТ/parsing/data\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n",
      "Пауза на 2\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ranil/Рабочий стол/Project/ПРОЕКТ/parsing/data/Балка б/у'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[340]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m worker = ParserSite_23MET(\u001b[33m\"\u001b[39m\u001b[33mhttps://23met.ru/price\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m worker.parsing(dir_name= \u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[339]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mParserSite_23MET.parsing\u001b[39m\u001b[34m(self, file_name_for_main_site, accept, dir_name, with_save_data_in_file)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_save_data_in_file:\n\u001b[32m     79\u001b[39m     \u001b[38;5;66;03m# Сохраняем все полученные данные в файлы\u001b[39;00m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, detail_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(hrefs_next_sites.keys()):\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.put_file(path= os.path.join(path, detail_name),\n\u001b[32m     82\u001b[39m                             data= html_codes[i])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[338]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mParser.put_file\u001b[39m\u001b[34m(self, path, data)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mput_file\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m, data: Any):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__file_worker.put(path= path, data= data)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[337]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mWorkerWithFiles.put\u001b[39m\u001b[34m(self, path, data)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mput\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m, data: Any):\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aiofiles.open(file= path, mode= \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m     18\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m file.write(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/parsing/lib/python3.13/site-packages/aiofiles/base.py:78\u001b[39m, in \u001b[36m_ContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28mself\u001b[39m._obj = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._coro\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/parsing/lib/python3.13/site-packages/aiofiles/threadpool/__init__.py:80\u001b[39m, in \u001b[36m_open\u001b[39m\u001b[34m(file, mode, buffering, encoding, errors, newline, closefd, opener, loop, executor)\u001b[39m\n\u001b[32m     68\u001b[39m     loop = asyncio.get_event_loop()\n\u001b[32m     69\u001b[39m cb = partial(\n\u001b[32m     70\u001b[39m     sync_open,\n\u001b[32m     71\u001b[39m     file,\n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m     opener=opener,\n\u001b[32m     79\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m f = \u001b[38;5;28;01myield from\u001b[39;00m loop.run_in_executor(executor, cb)\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(f, loop=loop, executor=executor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/parsing/lib/python3.13/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/ranil/Рабочий стол/Project/ПРОЕКТ/parsing/data/Балка б/у'"
     ]
    }
   ],
   "source": [
    "worker = ParserSite_23MET(\"https://23met.ru/price\")\n",
    "await worker.parsing(dir_name= 'data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parsing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
