{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0156315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import  ABC, abstractmethod\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import os\n",
    "from fake_useragent import UserAgent\n",
    "from typing import Dict, Any\n",
    "import aiofiles\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "036e075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Readable(ABC):\n",
    "    def __init__(self): pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get(self): pass\n",
    "\n",
    "class Writable(ABC):\n",
    "    def __init__(self): pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def put(self, data: Any): pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f72747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkerWithHtml(Readable):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._user = UserAgent().random\n",
    "        \n",
    "    @property\n",
    "    def user(self):\n",
    "        return  self._user\n",
    "\n",
    "    async def get(self, \n",
    "                  session: aiohttp.ClientSession, \n",
    "                  url: str,\n",
    "                  semaphore: asyncio.Semaphore= None,\n",
    "                  accept: str= '*/*'):\n",
    "        \n",
    "\n",
    "        header = {'Accept' : accept, \n",
    "                  'User-Agent': self._user,\n",
    "                  \"Accept-Language\": \"ru,en;q=0.9\",\n",
    "                  \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "                  \"Connection\": \"keep-alive\",\n",
    "                  \"Upgrade-Insecure-Requests\": \"1\",\n",
    "                  \"Sec-Fetch-Dest\": \"document\",\n",
    "                  \"Sec-Fetch-Mode\": \"navigate\",\n",
    "                  \"Sec-Fetch-Site\": \"none\",\n",
    "                  \"Sec-Fetch-User\": \"?1\",\n",
    "                  \"Referer\": \"https://23met.ru/\"}\n",
    "        \n",
    "\n",
    "        async def read_data_in_site():\n",
    "            data = None\n",
    "            if semaphore:\n",
    "                async with semaphore:\n",
    "                    # Чтение данных из сайта\n",
    "                    async with session.get(url= url, \n",
    "                                        headers= header) as response:\n",
    "                        data = await response.text()\n",
    "                        \n",
    "            else:\n",
    "                # Чтение данных из сайта\n",
    "                async with session.get(url= url, \n",
    "                                        headers= header) as response:\n",
    "                    data = await response.text()\n",
    "            return data\n",
    "\n",
    "        data = await read_data_in_site()\n",
    "        while BeautifulSoup(data, 'lxml').find('title').text == 'Слишком много запросов':\n",
    "            print(\"Блокировка! Жду 2 минуты и пробую снова...\")\n",
    "            header['User-Agent'] = UserAgent().random\n",
    "            await asyncio.sleep(120)\n",
    "            data = await read_data_in_site()\n",
    "\n",
    "        await asyncio.sleep(random.uniform(1, 3))\n",
    "        return data\n",
    "\n",
    "    async def _read_main_site(self, url: str, accept: str= \"*/*\"):\n",
    "\n",
    "        # Забираем данные с основного сайта и кладем их в файла \n",
    "        data = None\n",
    "        semaphore = asyncio.Semaphore(1)\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            data = await self.get(session= session, \n",
    "                                  url= url, \n",
    "                                  semaphore= semaphore,\n",
    "                                  accept= accept)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcb0e9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkerWithFiles(Readable, Writable):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    async def get(self, path: str):\n",
    "        data = None\n",
    "        async with aiofiles.open(file= path) as file:\n",
    "            data = await file.read()\n",
    "        return data\n",
    "    \n",
    "    async def put(self, path: str, data: Any):\n",
    "        async with aiofiles.open(file= path, mode= 'w') as file:\n",
    "            await file.write(data)\n",
    "    \n",
    "    async def _put_json_file(self, path: str, data: Any):\n",
    "        async with aiofiles.open(path, 'w') as file:\n",
    "            json_str = json.dumps(obj= data, indent= 4, ensure_ascii= False) \n",
    "            await file.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f355c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(ABC):\n",
    "    def __init__(self, base_url: str):\n",
    "        self.__file_worker = WorkerWithFiles()\n",
    "        self.__html_worker = WorkerWithHtml()\n",
    "        self.base_url = base_url\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def parsing(): pass\n",
    "\n",
    "    @property\n",
    "    def user_agent(self):\n",
    "        return self.__html_worker.user\n",
    "    \n",
    "    async def put_file(self, path: str, data: Any):\n",
    "        await self.__file_worker.put(path= path, data= data)\n",
    "    \n",
    "\n",
    "    async def get_file(self, path: str):\n",
    "        return await self.__file_worker.get(path= path)\n",
    "\n",
    "    \n",
    "    async def get_html(self, \n",
    "                       session: aiohttp.ClientSession,\n",
    "                       url: str,\n",
    "                       semaphore: asyncio.Semaphore= None,\n",
    "                       accept: str= '*/*'):\n",
    "        return await self.__html_worker.get(session= session, url= url, semaphore= semaphore, accept= accept)\n",
    "    \n",
    "\n",
    "    async def _read_main_site_and_save(self, \n",
    "                                       path_main_site: str, \n",
    "                                       file_name_for_main_site: str,\n",
    "                                       accept: str= \"*/*\"):\n",
    "        \n",
    "        data = await self.__html_worker._read_main_site(self.base_url, \n",
    "                                                        accept= accept)\n",
    "        \n",
    "        # # сохраняет данные в файл\n",
    "        # if BeautifulSoup(data, 'lxml').find(name= 'title').text == \"Слишком много запросов\":\n",
    "        #     raise Exception(\"Вас заблокировали!\")\n",
    "        \n",
    "        path= os.path.join(path_main_site, file_name_for_main_site)\n",
    "        await self.put_file(path= path, data= data)\n",
    "        \n",
    "\n",
    "    async def _save_data_in_json_file(self, path: str, data: Any):\n",
    "        await self.__file_worker._put_json_file(path, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6a177e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParserSite_23MET(Parser):\n",
    "    def __init__(self, base_url):\n",
    "        super().__init__(base_url)\n",
    "\n",
    "    async def __get_hrefs_for_next_sites(self,\n",
    "                                         file_path: str):\n",
    "\n",
    "        #сначало парсим основной сайт\n",
    "        html_file = await self.get_file(path= file_path)\n",
    "        soup = BeautifulSoup(markup= html_file, \n",
    "                             features= 'lxml')\n",
    "        \n",
    "        items_html_with_href= soup.find(name= 'nav', \n",
    "                                        attrs={\"id\" : \"left-container\", \"class\" : \"left-container-mainpage-static\"})\\\n",
    "                                  .find(name= 'ul')\\\n",
    "                                  .find_all(name= 'a')\n",
    "        \n",
    "        # формируем json с ссылками на сайты, внутри которых будут еще одни сайты с сылками на данные\n",
    "        hrefs_next_sites = dict()\n",
    "        for item in items_html_with_href:\n",
    "            item: BeautifulSoup\n",
    "            item_name = item.get_text()\n",
    "            item_href = self.base_url + item.get('href').replace('/price', '')\n",
    "            hrefs_next_sites[item_name] = item_href\n",
    "\n",
    "        return hrefs_next_sites\n",
    "\n",
    "    \n",
    "\n",
    "    # async def __sub_get_hrefs_for_next_sites(self, \n",
    "    #                                          file_path: str):\n",
    "    #     html_file = await self.get_file(path= file_path)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def sanitize_filename(filename: str) -> str:\n",
    "        # Заменяем все недопустимые символы на \"_\"\n",
    "        return re.sub(r'[\\\\/:\"*?<>|]+', '_', filename)\n",
    "    \n",
    "    async def _fetch_and_save_one(self, session, semaphore, url, path, detail_name, accept):\n",
    "        \"\"\"Скачивает и сразу сохраняет одну страницу.\"\"\"\n",
    "\n",
    "        async with semaphore:\n",
    "            print(f\"Начинаю обработку {detail_name}...\")\n",
    "            html = await self.get_html(session=session, url=url, accept=accept)\n",
    "\n",
    "            if html:\n",
    "                safe_name = self.sanitize_filename(filename= detail_name)\n",
    "                file_path = os.path.join(path, f\"{safe_name}.html\")\n",
    "                await self.put_file(path=file_path, data=html)\n",
    "                print(f\"Сохранил {detail_name} в {file_path}\")\n",
    "            else:\n",
    "                print(f\"Не удалось скачать {detail_name} с URL: {url}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    async def parsing(self,\n",
    "                      file_name_for_main_site= 'main_site.html',\n",
    "                      accept= '*/*',\n",
    "                      dir_name= None,\n",
    "                      MAX_TASKS= 5): \n",
    "        \n",
    "        path = os.getcwd()\n",
    "\n",
    "        # создаю директорию, если есть dir_name\n",
    "        if dir_name:\n",
    "            path = os.path.join(path, dir_name)\n",
    "            if not os.path.isdir(path):\n",
    "                os.mkdir(path= path)\n",
    "                print(f\"Создал папку {dir_name} по пути {path}\")\n",
    "            else:\n",
    "                print(f\"Не стал создавать папку {dir_name}, т.к она уже существует!\")\n",
    "        \n",
    "        try:\n",
    "            await self._read_main_site_and_save(path_main_site= path, \n",
    "                                                file_name_for_main_site= file_name_for_main_site,\n",
    "                                                accept= accept)\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            return ex\n",
    "\n",
    "\n",
    "        # Получаем json с ссылками на сайты\n",
    "        hrefs_next_sites = await self.__get_hrefs_for_next_sites(os.path.join(path, file_name_for_main_site)) \n",
    "\n",
    "        semaphore = asyncio.Semaphore(MAX_TASKS)\n",
    "        \n",
    "        # создадим отдельную директиву для сайтов\n",
    "        SUBMAIN_DIR_NAME = 'submain_sites'\n",
    "        dir_path = os.path.join(path, SUBMAIN_DIR_NAME) \n",
    "        counter= 0\n",
    "        while os.path.isdir(dir_path):\n",
    "            dir_path += str(counter)\n",
    "            counter += 1\n",
    "        os.mkdir(dir_path)\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            tasks = []\n",
    "            for detail_name, submain_url in hrefs_next_sites.items():\n",
    "                task = asyncio.create_task(\n",
    "                    self._fetch_and_save_one(session, semaphore, submain_url, dir_path, detail_name, accept)\n",
    "                )\n",
    "                tasks.append(task)\n",
    "            await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02192660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создал папку data по пути /home/ranil/Рабочий стол/Project/ПРОЕКТ/parsing/data\n",
      "Начинаю обработку Арматура А1...\n",
      "Начинаю обработку Арматура А1 оц....\n",
      "Начинаю обработку Арматура А3...\n",
      "Начинаю обработку Арматура А3 оц....\n",
      "Начинаю обработку Арматура Ат800...\n",
      "Сохранил Арматура А3 оц. в /home/ranil/Рабочий стол/Project/ПРОЕКТ/parsing/data/submain_sites/Арматура А3 оц..html\n",
      "Начинаю обработку Арматура Ат1000...\n",
      "Сохранил Арматура А3 в /home/ranil/Рабочий стол/Project/ПРОЕКТ/parsing/data/submain_sites/Арматура А3.html\n",
      "Начинаю обработку Арматура СП...\n",
      "Сохранил Арматура А1 оц. в /home/ranil/Рабочий стол/Project/ПРОЕКТ/parsing/data/submain_sites/Арматура А1 оц..html\n",
      "Начинаю обработку Балка...\n",
      "Сохранил Арматура Ат800 в /home/ranil/Рабочий стол/Project/ПРОЕКТ/parsing/data/submain_sites/Арматура Ат800.html\n",
      "Начинаю обработку Балка б/у...\n",
      "Сохранил Арматура А1 в /home/ranil/Рабочий стол/Project/ПРОЕКТ/parsing/data/submain_sites/Арматура А1.html\n",
      "Начинаю обработку Бочонок...\n",
      "Сохранил Арматура Ат1000 в /home/ranil/Рабочий стол/Project/ПРОЕКТ/parsing/data/submain_sites/Арматура Ат1000.html\n",
      "Начинаю обработку Бочонок оцинк....\n",
      "Сохранил Балка в /home/ranil/Рабочий стол/Project/ПРОЕКТ/parsing/data/submain_sites/Балка.html\n",
      "Начинаю обработку Вентиль...\n",
      "Сохранил Балка б/у в /home/ranil/Рабочий стол/Project/ПРОЕКТ/parsing/data/submain_sites/Балка б_у.html\n",
      "Начинаю обработку Виброкомпенсатор...\n",
      "Сохранил Арматура СП в /home/ranil/Рабочий стол/Project/ПРОЕКТ/parsing/data/submain_sites/Арматура СП.html\n",
      "Начинаю обработку Втулка ПНД...\n",
      "Сохранил Бочонок оцинк. в /home/ranil/Рабочий стол/Project/ПРОЕКТ/parsing/data/submain_sites/Бочонок оцинк..html\n",
      "Начинаю обработку Днище...\n",
      "Сохранил Бочонок в /home/ranil/Рабочий стол/Project/ПРОЕКТ/parsing/data/submain_sites/Бочонок.html\n",
      "Начинаю обработку Заглушка...\n",
      "Сохранил Вентиль в /home/ranil/Рабочий стол/Project/ПРОЕКТ/parsing/data/submain_sites/Вентиль.html\n",
      "Начинаю обработку Заглушка оцинк....\n",
      "Сохранил Виброкомпенсатор в /home/ranil/Рабочий стол/Project/ПРОЕКТ/parsing/data/submain_sites/Виброкомпенсатор.html\n",
      "Начинаю обработку Заглушка ПНД...\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m worker = ParserSite_23MET(\u001b[33m\"\u001b[39m\u001b[33mhttps://23met.ru/price\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m worker.parsing(dir_name= \u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36mParserSite_23MET.parsing\u001b[39m\u001b[34m(self, file_name_for_main_site, accept, dir_name, MAX_TASKS)\u001b[39m\n\u001b[32m    102\u001b[39m     task = asyncio.create_task(\n\u001b[32m    103\u001b[39m         \u001b[38;5;28mself\u001b[39m._fetch_and_save_one(session, semaphore, submain_url, dir_path, detail_name, accept)\n\u001b[32m    104\u001b[39m     )\n\u001b[32m    105\u001b[39m     tasks.append(task)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mParserSite_23MET._fetch_and_save_one\u001b[39m\u001b[34m(self, session, semaphore, url, path, detail_name, accept)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m semaphore:\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mНачинаю обработку \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdetail_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     html = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_html(session=session, url=url, accept=accept)\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m html:\n\u001b[32m     48\u001b[39m         safe_name = \u001b[38;5;28mself\u001b[39m.sanitize_filename(filename= detail_name)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mParser.get_html\u001b[39m\u001b[34m(self, session, url, semaphore, accept)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_html\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[32m     24\u001b[39m                    session: aiohttp.ClientSession,\n\u001b[32m     25\u001b[39m                    url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m     26\u001b[39m                    semaphore: asyncio.Semaphore= \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     27\u001b[39m                    accept: \u001b[38;5;28mstr\u001b[39m= \u001b[33m'\u001b[39m\u001b[33m*/*\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__html_worker.get(session= session, url= url, semaphore= semaphore, accept= accept)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mWorkerWithHtml.get\u001b[39m\u001b[34m(self, session, url, semaphore, accept)\u001b[39m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m session.get(url= url, \n\u001b[32m     42\u001b[39m                             headers= header) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[32m     43\u001b[39m         data = \u001b[38;5;28;01mawait\u001b[39;00m response.text()\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(random.uniform(\u001b[32m5\u001b[39m, \u001b[32m15\u001b[39m))\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/parsing/lib/python3.13/asyncio/tasks.py:718\u001b[39m, in \u001b[36msleep\u001b[39m\u001b[34m(delay, result)\u001b[39m\n\u001b[32m    714\u001b[39m h = loop.call_later(delay,\n\u001b[32m    715\u001b[39m                     futures._set_result_unless_cancelled,\n\u001b[32m    716\u001b[39m                     future, result)\n\u001b[32m    717\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    720\u001b[39m     h.cancel()\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "worker = ParserSite_23MET(\"https://23met.ru/price\")\n",
    "await worker.parsing(dir_name= 'data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parsing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
