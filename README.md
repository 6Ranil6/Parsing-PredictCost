# Проект парсинга и обработки данных с сайта 23met.ru

Этот проект представляет собой комплексный асинхронный пайплайн для сбора, обработки и структурирования данных о металлопродукции с сайта `23met.ru`.

## Функционал

- **Поиск прокси-серверов**: Скрипт может автоматически находить и проверять бесплатные прокси-серверы для обхода блокировок.
- **Поиск страниц с прайс-листами**: Используется внешний API для поиска в Google всех релевантных страниц с прайс-листами на целевом сайте.
- **Асинхронный парсинг**: Все страницы скачиваются и обрабатываются асинхронно, что значительно ускоряет процесс.
- **Продвинутая предобработка данных**: После сбора "сырых" данных запускается мощный модуль предобработки, который:
  - Очищает и нормализует текстовые данные.
  - Извлекает структурированную информацию из сложных полей (например, "Размер", "Сталь", "ГОСТ").
  - Объединяет и преобразует столбцы с ценами.
  - Создает новые полезные признаки (Тип материала, Свариваемость, компоненты стандарта и т.д.).

## Структура проекта

- **`main.py`**: Главный исполняемый файл. Запускает и координирует работу всех модулей.
- **`parser_23MET.py`**: Основной парсер, отвечающий за скачивание и извлечение данных с сайта `23met.ru`.
- **`preProcessor.py`**: Модуль предобработки. Содержит класс `PreProcessor` для очистки и структурирования данных, полученных парсером.
- **`GoogleParser.py`**: Парсер, который использует `scraperapi.com` для поиска в Google и получения списка URL-адресов прайс-листов.
- **`proxyParser.py`**: Парсер для сбора и проверки бесплатных прокси-серверов с сайта `proxylib.com`.
- **`base.py`**: Содержит базовые абстрактные классы (`Parser`, `Readable`, `Writable`) и вспомогательные классы для работы с файлами и HTTP-запросами.
- **`update_config.py`**: Утилита для управления файлом конфигурации `config.json`, в частности, для отслеживания использования API-ключей и дат обновления.
- **`config.json`**: Файл конфигурации для хранения API-ключей и настроек обновления.

## Установка и запуск

1.  **Клонируйте репозиторий:**
    ```bash
    git clone <your-repository-url>
    cd <repository-directory>
    ```

2.  **Установите зависимости:**
    Убедитесь, что у вас установлен Python 3.8+.
    ```bash
    pip install -r requirements.txt
    ```

3.  **Настройте `config.json`:**
    Создайте в корневой директории проекта файл `config.json`. Он необходим для работы `GoogleParser`.

    **Пример `config.json`:**
    ```json
    {
        "UPDATE": {
            "LAST_CHECKING_DATE": "2023-01-01",
            "NEXT_UPDATE": 30
        },
        "API_KEYS": {
            "User_1": {
                "API_KEY": "ВАШ_API_КЛЮЧ_ОТ_SCRAPERAPI_1",
                "MAX_CREDIT": 5000,
                "USED_CREDIT": 0
            },
            "User_2": {
                "API_KEY": "ВАШ_API_КЛЮЧ_ОТ_SCRAPERAPI_2",
                "MAX_CREDIT": 5000,
                "USED_CREDIT": 0
            }
        }
    }
    ```
    - Замените `"ВАШ_API_КЛЮЧ_..."` на ваши реальные ключи от [scraperapi.com](https://www.scraperapi.com/). Можно использовать несколько ключей для увеличения лимита запросов.

4.  **Запустите скрипт:**
    ```bash
    python main.py
    ```
    В файле `main.py` можно изменить флаг `with_proxy=True`, чтобы использовать прокси-серверы.

## Результаты работы

- **`23MET_DATA/result.csv`**: "Сырые" данные, полученные сразу после парсинга.
- **`23MET_DATA/preprocessing_result.csv`**: Финальный файл с очищенными, обработанными и структурированными данными. Готов для дальнейшего анализа.


# Для автоматизации процесса сбора данных можно использовать утилиту Cron

Для этого необходимо:

1) Зайти в терминал и ввести команду: 

```bash
crontab -e
```

2) При первом запуске нужно выбрать текстовый редактор, который вам предпочтителен для изменения ПОЛЬЗОВАТЕЛЬСКОЙ ТАБЛИЦЫ CRON.

3) Далее нужно ввести:

```bash
0 12 * * * [АБСОЛЮТНЫЙ ПУТЬ ДЛЯ ИНТЕРПРЕТАТОРА PYTHON`][`АБСОЛЮТНЫЙ ПУТЬ ДО main.py СКЛОНИРОВАННОГО РЕПОЗИТОРИЯ НА ВАШЕМ ХОСТЕ`] >> [`АБСОЛЮТНЫЙ ПУТЬ ДО cron.log СКЛОНИРОВАННОГО РЕПОЗИТОРИЯ НА ВАШЕМ ХОСТЕ`] 2>&1
```

**Итог:** ваш код будет запускаться каждый день в 12:00, но сервер или ваш ПК должен быть включенным в это время


*Примечание*: В папке АНАЛИЗ_И_ИЗУЧЕНИЕ можно найти все источники и мои конспекты, которые помогут понять или вспомнить нужный материал для выполнения аналогичной задачи. 

